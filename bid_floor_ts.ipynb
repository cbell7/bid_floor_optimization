{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import [internal library]\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "from darts import TimeSeries\n",
    "#import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redacted\n",
    "# Use internal libraries for database connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average CPC and ROAS query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redacted\n",
    "# Query to pull average CPC and ROAS by producct taxonomy\n",
    "cpc_query = \"\"\n",
    "cpc_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bid Floors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redacted\n",
    "bid_floor_query = \"\"\n",
    "bid_floor_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Categories\n",
    "Using level 1 and level 2 taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTD Metrics\n",
    "year_start = datetime.datetime(2024, 1, 1).date()\n",
    "cpc_ytd = cpc_data[cpc_data[\"month\"].dt.date >= year_start]\n",
    "\n",
    "# Filter categories by ROAS\n",
    "roas = (cpc_ytd\n",
    " .groupby([\"taxonomy_l1\", \"taxonomy_l2\"])\n",
    " [[\"sales\", \"spend\"]]\n",
    " .sum()\n",
    " .reset_index()\n",
    " .assign(roas=lambda x: x.sales / x.spend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test categories: choose only categories with high ROAS (above roas_lim)\n",
    "roas_lim = 4 # very conservative\n",
    "roas_filtered = roas.query(f'roas > {roas_lim}')\n",
    "test_categories = roas_filtered[[\"taxonomy_l1\", \"taxonomy_l2\"]].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to test categories\n",
    "cpc_data_filtered = cpc_data.merge(test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to bid floors, then aggregate\n",
    "cpc_data_final = (cpc_data_filtered\n",
    " .merge(bid_floor_data)\n",
    " .groupby([\"month\", \"taxonomy_l1\", \"taxonomy_l2\"])\n",
    " .agg({\n",
    "     \"clicks\": \"sum\",\n",
    "     \"spend\": \"sum\",\n",
    "     \"min_cpc\": \"mean\"\n",
    " })\n",
    " .reset_index()\n",
    " .assign(avg_cpc = lambda x: x.spend / x.clicks)\n",
    " )\n",
    "cpc_data_final[\"month\"] = cpc_data_final[\"month\"].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Time Series Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import StatsForecastAutoARIMA\n",
    "from darts.metrics import mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate forecast function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def generate_forecast(test_index):\n",
    "    \n",
    "    try:\n",
    "        idx = test_index\n",
    "        \n",
    "        # Create data frame for single time series\n",
    "        category = test_categories[test_categories.index == idx].reset_index()\n",
    "        ts_df = cpc_data_final.merge(category)\n",
    "        ts_df = ts_df.drop(ts_df.tail(1).index) # Drop current month\n",
    "        tax_name = category[\"taxonomy_l1\"][0] + ' > ' + category[\"taxonomy_l2\"][0]\n",
    "    \n",
    "    except:\n",
    "        print(f\"Failed to create forecast DataFrame for category index {test_index}\")\n",
    "    \n",
    "    try:\n",
    "        # Create monthly time series\n",
    "        ts = TimeSeries.from_dataframe(ts_df, time_col='month', freq='MS', value_cols=['avg_cpc'])\n",
    "        \n",
    "        # Training data is first two years\n",
    "        train, val = ts.split_before(pd.Timestamp('2023-08-01'))\n",
    "        \n",
    "        # Fit model to training data and store MAPE\n",
    "        arima_train = StatsForecastAutoARIMA()\n",
    "        arima_train.fit(train)\n",
    "        forecast = arima_train.predict(len(val), num_samples=1000)\n",
    "        validation_mape = mape(forecast, val) / 100 \n",
    "        \n",
    "        # Fit full model\n",
    "        arima = StatsForecastAutoARIMA()\n",
    "        arima.fit(ts)\n",
    "        forecast = arima.predict(3, num_samples=1000)\n",
    "        \n",
    "        # Extract forecast estimates and confidence intervals\n",
    "        hilo = forecast.quantiles_df((0.05, 0.5, 0.95)) # 90% CI\n",
    "        hilo[\"taxonomy_l1\"] = category[\"taxonomy_l1\"][0]\n",
    "        hilo[\"taxonomy_l2\"] = category[\"taxonomy_l2\"][0]\n",
    "        hilo[\"bid_floor\"] = ts_df.sort_values(\"month\").tail(1)[\"min_cpc\"].reset_index(drop=True)[0]\n",
    "        hilo[\"mape\"] = validation_mape\n",
    "\n",
    "        # Aggregate values over next quarter   \n",
    "        hilo_agg = (hilo\n",
    "            .groupby([\"taxonomy_l1\", \"taxonomy_l2\", \"bid_floor\", \"mape\"])\n",
    "            .agg({\"avg_cpc_0.05\": \"min\", \n",
    "                \"avg_cpc_0.5\": \"median\", \n",
    "                \"avg_cpc_0.95\":\"max\"})\n",
    "            .reset_index()\n",
    "            .rename(columns={\n",
    "                \"avg_cpc_0.05\": \"lower_bound\", \n",
    "                \"avg_cpc_0.5\": \"est\", \n",
    "                \"avg_cpc_0.95\": \"upper_bound\"\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        return hilo_agg\n",
    "    \n",
    "    except:\n",
    "        print(f\"Failed to create forecast for {tax_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop: attempt to generate forecasts for all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:main_logger:ValueError: Train series only contains 1 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for car electronics & gps > car video & dvd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:darts.timeseries:ValueError: Timestamp must be between 2023-11-01 00:00:00 and 2024-01-01 00:00:00\n",
      "ERROR:main_logger:ValueError: Train series only contains 5 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for cell phones > lively phones\n",
      "Failed to create forecast for cell phones > samsung galaxy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:darts.timeseries:ValueError: Timestamp must be between 2022-05-01 00:00:00 and 2023-07-01 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for electric transportation > safety gear & accessories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:main_logger:ValueError: Train series only contains 9 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for home, furniture & office > tools & garage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:main_logger:ValueError: Train series only contains 1 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for smart home > smart home hubs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:main_logger:ValueError: Train series only contains 1 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for video games > all video games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:main_logger:ValueError: Train series only contains 0 elements but StatsForecastAutoARIMA() model requires at least 10 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create forecast for video games > virtual reality\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(test_categories.index) - 1):\n",
    "    fcast = generate_forecast(i)\n",
    "    df = pd.concat([df, fcast])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Final Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated quarterly forecasts for 61 of 108 categories (56%) with ROAS > 4 and MAPE < 30%!\n"
     ]
    }
   ],
   "source": [
    "# Remove high volatility categories we can't forecast well\n",
    "mape_limit = 0.3\n",
    "\n",
    "# Create final DataFrame\n",
    "df_final = (df\n",
    "            # Negative lower bound isn't meaningful, so set to zero\n",
    "            .assign(lower_bound = lambda x: np.where(x.lower_bound < 0, 0, x.lower_bound))\n",
    "            .query(f'mape < {mape_limit}')\n",
    "            .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "n_categories_final = len(df_final.index)\n",
    "n_categories_start = len(roas.index)\n",
    "forecasted_category_pct = round(n_categories_final / n_categories_start * 100)\n",
    "print(f'Successfully generated quarterly forecasts for {n_categories_final} of {n_categories_start} categories ({forecasted_category_pct}%) with ROAS > {roas_lim} and MAPE < {round(mape_limit * 100)}%!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(f'output/taxonomy_l2_forecasts_bid_floors_{retailer_id}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
